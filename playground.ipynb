{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Baseline model\n",
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')\n",
    "maxLen = len(max(X_train, key=len).split())#because we have to embed 0 for other short sentences\n",
    "\n",
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)\n",
    "\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "\twords = (sentence.lower()).split()\n",
    "\tavg = np.zeros([50,])#because word_to_vec_map is 50-dimensional\n",
    "\tfor w in words:\n",
    "\t\tavg += word_to_vec_map[w]\n",
    "\t\tavg = avg/len(words)\n",
    "\n",
    "\treturn avg\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "\tnp.random.seed(1)\n",
    "\tm = Y.shape[0]\n",
    "\tn_y = 5\n",
    "\tn_h = 50\n",
    "\n",
    "\tW = np.random.randn(n_y, n_h) / np.sqrt(n_h) # initialization\n",
    "\tb = np.zeros((n_y,))\n",
    "\n",
    "\t#convert Y to Y_onehot\n",
    "\tY_oh = convert_to_one_hot(Y, C=n_y)\n",
    "\n",
    "\tfor t in range(num_iterations):\n",
    "\t\tfor i in range(m):\n",
    "\t\t\tavg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "\t\t\tz = np.dot(W, avg) + b\n",
    "\t\t\ta = softmax(z) # same shape with Y_oh\n",
    "\n",
    "\t\t\tcost = -np.sum(Y_oh[i]*np.log10(a))\n",
    "\n",
    "\t\t\tdz = a - Y_oh[i]\n",
    "\t\t\tdW = np.dot(dz.reshape(n_y, 1), avg.reshape(1, n_h))\n",
    "\t\t\tdb = dz\n",
    "\n",
    "\t\t\tW = W - learning_rate * dW\n",
    "\t\t\tb = b - learning_rate * db\n",
    "\n",
    "\n",
    "\t\tif t%100 == 0:\n",
    "\t\t\tprint(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "\t\t\tpred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\treturn pred, W, b\n",
    "\n",
    "ped, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you ‚ù§Ô∏è\n",
      "i love you ‚ù§Ô∏è\n",
      "funny lol üòÑ\n",
      "lets play with a ball ‚öæ\n",
      "food is ready üç¥\n",
      "not feeling happy ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USIng LSTM in keras\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "\tm = X.shape[0]\n",
    "\tX_indices = np.zeros([m, max_len])\n",
    "\tfor i in range(m):\n",
    "\t\tsentence_words = (X[i].lower()).split()\n",
    "\t\tj = 0\n",
    "\t\tfor w in sentence_words:\n",
    "\t\t\tX_indices[i, j] = word_to_index[w]\n",
    "\t\t\tj = j + 1\n",
    "\n",
    "\treturn X_indices\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "\tvocab_len = len(word_to_index) + 1\n",
    "\temb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "\temb_matrix = np.zeros([vocab_len, emb_dim])\n",
    "\tfor word, index in word_to_index.items():\n",
    "\t\temb_matrix[index, :] = word_to_vec_map[word]\n",
    "\t#define keras embedding layer\n",
    "\tembedding_layer = Embedding(vocab_len, emb_dim, \n",
    "\t\tembeddings_initializer='uniform', trainable = False)\n",
    "\t# has to use this build before set_weights\n",
    "\tembedding_layer.build((None,))\n",
    "\t# kind of initialization\n",
    "\tembedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "\treturn embedding_layer\n",
    "\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "\tsentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "\tembedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\tembeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "\t# Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "\t#Âú®ËæìÂá∫Â∫èÂàó‰∏≠ÔºåËøîÂõûÂçï‰∏™ hidden stateÂÄºËøòÊòØËøîÂõûÂÖ®ÈÉ®time step ÁöÑ hidden stateÂÄº„ÄÇ False ËøîÂõûÂçï‰∏™Ôºå true ËøîÂõûÂÖ®ÈÉ®„ÄÇ\n",
    "\t# 128Ôºödimensionality of the output space.\n",
    "\t# if return_sequence=True, it will output all hidden states\n",
    "\tX = LSTM(128, return_sequences=True)(embeddings)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = LSTM(128, return_sequences=False)(X)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = Dense(5)(X)\n",
    "\tX = Activation(\"softmax\")(X)\n",
    "\n",
    "\tmodel = Model(inputs=sentence_indices, outputs=X)\n",
    "\n",
    "\treturn model\n",
    "#max_len = 5\n",
    "\n",
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle = True)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)\n",
    "\n",
    "#test\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test_indices)):\n",
    "\tx = X_test_indices\n",
    "\tnum = np.argmax(pred[i])\n",
    "\tif(num != Y_test[i]):\n",
    "\t\tprint('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòû\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['not feeling happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "\twith open(glove_file, 'r') as f:\n",
    "\t\twords = set()\n",
    "\t\tword_to_vec_map = {}# dictionary\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip().split()#glove is a file contains word and the corresponding vector\n",
    "\t\t\tcurr_word = line[0]\n",
    "\t\t\twords.add(curr_word)\n",
    "\t\t\tword_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\t\t\n",
    "\t\ti = 1\n",
    "\t\twords_to_index = {}\n",
    "\t\tindex_to_words = {}\n",
    "\t\tfor w in sorted(words):\n",
    "\t\t\twords_to_index[w] = i\n",
    "\t\t\tindex_to_words[i] = w\n",
    "\t\t\ti = i + 1\n",
    "\treturn words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def softmax(x):\n",
    "\t\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "\te_x = np.exp(x - np.max(x))\n",
    "\treturn e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "\tphrase = []\n",
    "\temoji = []\n",
    "\n",
    "\twith open (filename) as csvDataFile:\n",
    "\t\tcsvReader = csv.reader(csvDataFile)\n",
    "\n",
    "\t\tfor row in csvReader:\n",
    "\t\t\tphrase.append(row[0])\n",
    "\t\t\temoji.append(row[1])\n",
    "\n",
    "\tX = np.asarray(phrase)\n",
    "\tY = np.asarray(emoji, dtype=int)\n",
    "\n",
    "\treturn X, Y\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "\tY = np.eye(C)[Y.reshape(-1)] # will convert every element in Y to a vector\n",
    "\treturn Y\n",
    "\n",
    "\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "\t\t\t\t\t\"1\": \":baseball:\",\n",
    "\t\t\t\t\t\"2\": \":smile:\",\n",
    "\t\t\t\t\t\"3\": \":disappointed:\",\n",
    "\t\t\t\t\t\"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "\t\"\"\"\n",
    "\tConverts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "\t\"\"\"\n",
    "\treturn emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\t\t\t  \n",
    "\t\n",
    "def print_predictions(X, pred):\n",
    "\tprint()\n",
    "\tfor i in range(X.shape[0]):\n",
    "\t\tprint(X[i], label_to_emoji(int(pred[i])))\n",
    "\t\t\n",
    "\t\t\n",
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "\t\n",
    "\tdf_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\t\n",
    "\tdf_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "\t\n",
    "\tplt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "\t#plt.title(title)\n",
    "\tplt.colorbar()\n",
    "\ttick_marks = np.arange(len(df_confusion.columns))\n",
    "\tplt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "\tplt.yticks(tick_marks, df_confusion.index)\n",
    "\t#plt.tight_layout()\n",
    "\tplt.ylabel(df_confusion.index.name)\n",
    "\tplt.xlabel(df_confusion.columns.name)\n",
    "\t\n",
    "\t\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "\t\"\"\"\n",
    "\tGiven X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "\t\n",
    "\tArguments:\n",
    "\tX -- input data containing sentences, numpy array of shape (m, None)\n",
    "\tY -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "\t\n",
    "\tReturns:\n",
    "\tpred -- numpy array of shape (m, 1) with your predictions\n",
    "\t\"\"\"\n",
    "\tm = X.shape[0]\n",
    "\tpred = np.zeros((m, 1))\n",
    "\t\n",
    "\tfor j in range(m):                       # Loop over training examples\n",
    "\t\t\n",
    "\t\t# Split jth test example (sentence) into list of lower case words\n",
    "\t\twords = X[j].lower().split()\n",
    "\t\t\n",
    "\t\t# Average words' vectors\n",
    "\t\tavg = np.zeros((50,))\n",
    "\t\tfor w in words:\n",
    "\t\t\tavg += word_to_vec_map[w]\n",
    "\t\tavg = avg/len(words)\n",
    "\n",
    "\t\t# Forward propagation\n",
    "\t\tZ = np.dot(W, avg) + b\n",
    "\t\tA = softmax(Z)\n",
    "\t\tpred[j] = np.argmax(A)\n",
    "\t\t\n",
    "\tprint(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "\t\n",
    "\treturn pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
