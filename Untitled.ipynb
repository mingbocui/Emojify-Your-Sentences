{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Baseline model\n",
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')\n",
    "maxLen = len(max(X_train, key=len).split())#because we have to embed 0 for other short sentences\n",
    "\n",
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)\n",
    "\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "\twords = (sentence.lower()).split()\n",
    "\tavg = np.zeros([50,])#because word_to_vec_map is 50-dimensional\n",
    "\tfor w in words:\n",
    "\t\tavg += word_to_vec_map[w]\n",
    "\t\tavg = avg/len(words)\n",
    "\n",
    "\treturn avg\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "\tnp.random.seed(1)\n",
    "\tm = Y.shape[0]\n",
    "\tn_y = 5\n",
    "\tn_h = 50\n",
    "\n",
    "\tW = np.random.randn(n_y, n_h) / np.sqrt(n_h) # initialization\n",
    "\tb = np.zeros((n_y,))\n",
    "\n",
    "\t#convert Y to Y_onehot\n",
    "\tY_oh = convert_to_one_hot(Y, C=n_y)\n",
    "\n",
    "\tfor t in range(num_iterations):\n",
    "\t\tfor i in range(m):\n",
    "\t\t\tavg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "\t\t\tz = np.dot(W, avg) + b\n",
    "\t\t\ta = softmax(z) # same shape with Y_oh\n",
    "\n",
    "\t\t\tcost = -np.sum(Y_oh[i]*np.log10(a))\n",
    "\n",
    "\t\t\tdz = a - Y_oh[i]\n",
    "\t\t\tdW = np.dot(dz.reshape(n_y, 1), avg.reshape(1, n_h))\n",
    "\t\t\tdb = dz\n",
    "\n",
    "\t\t\tW = W - learning_rate * dW\n",
    "\t\t\tb = b - learning_rate * db\n",
    "\n",
    "\n",
    "\t\tif t%100 == 0:\n",
    "\t\t\tprint(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "\t\t\tpred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\treturn pred, W, b\n",
    "\n",
    "ped, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you ‚ù§Ô∏è\n",
      "i love you ‚ù§Ô∏è\n",
      "funny lol üòÑ\n",
      "lets play with a ball ‚öæ\n",
      "food is ready üç¥\n",
      "not feeling happy ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USIng LSTM in keras\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "\tm = X.shape[0]\n",
    "\tX_indices = np.zeros([m, max_len])\n",
    "\tfor i in range(m):\n",
    "\t\tsentence_words = (X[i].lower()).split()\n",
    "\t\tj = 0\n",
    "\t\tfor w in sentence_words:\n",
    "\t\t\tX_indices[i, j] = word_to_index[w]\n",
    "\t\t\tj = j + 1\n",
    "\n",
    "\treturn X_indices\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "\tvocab_len = len(word_to_index) + 1\n",
    "\temb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "\temb_matrix = np.zeros([vocab_len, emb_dim])\n",
    "\tfor word, index in word_to_index.items():\n",
    "\t\temb_matrix[index, :] = word_to_vec_map[word]\n",
    "\t#define keras embedding layer\n",
    "\tembedding_layer = Embedding(vocab_len, emb_dim, \n",
    "\t\tembeddings_initializer='uniform', trainable = False)\n",
    "\t# has to use this build before set_weights\n",
    "\tembedding_layer.build((None,))\n",
    "\t# kind of initialization\n",
    "\tembedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "\treturn embedding_layer\n",
    "\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "\tsentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "\tembedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\tembeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "\t# Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "\t#Âú®ËæìÂá∫Â∫èÂàó‰∏≠ÔºåËøîÂõûÂçï‰∏™ hidden stateÂÄºËøòÊòØËøîÂõûÂÖ®ÈÉ®time step ÁöÑ hidden stateÂÄº„ÄÇ False ËøîÂõûÂçï‰∏™Ôºå true ËøîÂõûÂÖ®ÈÉ®„ÄÇ\n",
    "\t# 128Ôºödimensionality of the output space.\n",
    "\t# if return_sequence=True, it will output all hidden states\n",
    "\tX = LSTM(128, return_sequences=True)(embeddings)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = LSTM(128, return_sequences=False)(X)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = Dense(5)(X)\n",
    "\tX = Activation(\"softmax\")(X)\n",
    "\n",
    "\tmodel = Model(inputs=sentence_indices, outputs=X)\n",
    "\n",
    "\treturn model\n",
    "#max_len = 5\n",
    "\n",
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 1.6017 - acc: 0.1818\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 616us/step - loss: 1.5348 - acc: 0.2727\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 602us/step - loss: 1.4850 - acc: 0.3030\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 615us/step - loss: 1.4257 - acc: 0.3712\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 602us/step - loss: 1.3335 - acc: 0.4848\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 613us/step - loss: 1.2071 - acc: 0.6061\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 604us/step - loss: 1.0462 - acc: 0.6667\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 603us/step - loss: 0.9119 - acc: 0.6515\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 606us/step - loss: 0.9036 - acc: 0.6818\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 607us/step - loss: 0.8135 - acc: 0.7045\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 604us/step - loss: 0.7125 - acc: 0.7727\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 602us/step - loss: 0.7582 - acc: 0.6818\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 606us/step - loss: 0.6272 - acc: 0.7652\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 599us/step - loss: 0.5227 - acc: 0.8409\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 607us/step - loss: 0.5203 - acc: 0.8030\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 612us/step - loss: 0.4253 - acc: 0.8030\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 621us/step - loss: 0.3689 - acc: 0.8788\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 611us/step - loss: 0.4147 - acc: 0.8409\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 608us/step - loss: 0.3642 - acc: 0.8409\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 601us/step - loss: 0.3170 - acc: 0.8864\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 605us/step - loss: 0.3326 - acc: 0.8788\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 607us/step - loss: 0.2489 - acc: 0.9167\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 605us/step - loss: 0.1912 - acc: 0.9470\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 610us/step - loss: 0.2188 - acc: 0.9242\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 602us/step - loss: 0.1870 - acc: 0.9318\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 603us/step - loss: 0.1466 - acc: 0.9470\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 615us/step - loss: 0.1667 - acc: 0.9545\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 603us/step - loss: 0.1483 - acc: 0.9545\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 600us/step - loss: 0.2344 - acc: 0.9015\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 602us/step - loss: 0.2912 - acc: 0.9091\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 603us/step - loss: 0.3218 - acc: 0.9318\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 609us/step - loss: 0.1635 - acc: 0.9394\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 612us/step - loss: 0.1411 - acc: 0.9621\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 611us/step - loss: 0.1136 - acc: 0.9621\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 604us/step - loss: 0.1157 - acc: 0.9697\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 608us/step - loss: 0.5962 - acc: 0.8409\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 597us/step - loss: 0.4764 - acc: 0.8561\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 603us/step - loss: 0.1966 - acc: 0.9318\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 613us/step - loss: 0.3474 - acc: 0.8561\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 605us/step - loss: 0.1975 - acc: 0.9394\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 620us/step - loss: 0.1412 - acc: 0.9621\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 611us/step - loss: 0.1260 - acc: 0.9697\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 605us/step - loss: 0.1128 - acc: 0.9621\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 601us/step - loss: 0.0657 - acc: 0.9848\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 608us/step - loss: 0.0698 - acc: 0.9697\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 600us/step - loss: 0.0534 - acc: 0.9773\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 614us/step - loss: 0.0445 - acc: 0.9924\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 611us/step - loss: 0.0495 - acc: 0.9848\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 612us/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 611us/step - loss: 0.0271 - acc: 0.9924\n",
      "56/56 [==============================] - 0s 8ms/step\n",
      "\n",
      "Test accuracy =  0.7678571513720921\n",
      "Expected emoji:üòÑ prediction: he got a very nice raise\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: she got me a nice present\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is hard\tüòÑ\n",
      "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is horrible\tüòÑ\n",
      "Expected emoji:üç¥ prediction: any suggestions for dinner\tüòÑ\n",
      "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: My life is so boring\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
      "Expected emoji:‚öæ prediction: what is your favorite baseball game\tüòÑ\n",
      "Expected emoji:üòû prediction: go away\t‚öæ\n",
      "Expected emoji:üç¥ prediction: I did not have breakfast ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle = True)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)\n",
    "\n",
    "#test\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test_indices)):\n",
    "\tx = X_test_indices\n",
    "\tnum = np.argmax(pred[i])\n",
    "\tif(num != Y_test[i]):\n",
    "\t\tprint('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòû\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['not feeling happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_3 to have shape (5,) but got array with shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-91083829f806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m model.fit(X_train_indices, Y_train_oh, epochs = 50,\n\u001b[0;32m---> 72\u001b[0;31m \tbatch_size = 32, shuffle = True)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mX_test_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mY_test_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_3 to have shape (5,) but got array with shape (10,)"
     ]
    }
   ],
   "source": [
    "###############################################################################################\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "\twith open(glove_file, 'r') as f:\n",
    "\t\twords = set()\n",
    "\t\tword_to_vec_map = {}# dictionary\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip().split()#glove is a file contains word and the corresponding vector\n",
    "\t\t\tcurr_word = line[0]\n",
    "\t\t\twords.add(curr_word)\n",
    "\t\t\tword_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\t\t\n",
    "\t\ti = 1\n",
    "\t\twords_to_index = {}\n",
    "\t\tindex_to_words = {}\n",
    "\t\tfor w in sorted(words):\n",
    "\t\t\twords_to_index[w] = i\n",
    "\t\t\tindex_to_words[i] = w\n",
    "\t\t\ti = i + 1\n",
    "\treturn words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def softmax(x):\n",
    "\t\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "\te_x = np.exp(x - np.max(x))\n",
    "\treturn e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "\tphrase = []\n",
    "\temoji = []\n",
    "\n",
    "\twith open (filename) as csvDataFile:\n",
    "\t\tcsvReader = csv.reader(csvDataFile)\n",
    "\n",
    "\t\tfor row in csvReader:\n",
    "\t\t\tphrase.append(row[0])\n",
    "\t\t\temoji.append(row[1])\n",
    "\n",
    "\tX = np.asarray(phrase)\n",
    "\tY = np.asarray(emoji, dtype=int)\n",
    "\n",
    "\treturn X, Y\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "\tY = np.eye(C)[Y.reshape(-1)] # will convert every element in Y to a vector\n",
    "\treturn Y\n",
    "\n",
    "\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "\t\t\t\t\t\"1\": \":baseball:\",\n",
    "\t\t\t\t\t\"2\": \":smile:\",\n",
    "\t\t\t\t\t\"3\": \":disappointed:\",\n",
    "\t\t\t\t\t\"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "\t\"\"\"\n",
    "\tConverts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "\t\"\"\"\n",
    "\treturn emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\t\t\t  \n",
    "\t\n",
    "def print_predictions(X, pred):\n",
    "\tprint()\n",
    "\tfor i in range(X.shape[0]):\n",
    "\t\tprint(X[i], label_to_emoji(int(pred[i])))\n",
    "\t\t\n",
    "\t\t\n",
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "\t\n",
    "\tdf_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\t\n",
    "\tdf_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "\t\n",
    "\tplt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "\t#plt.title(title)\n",
    "\tplt.colorbar()\n",
    "\ttick_marks = np.arange(len(df_confusion.columns))\n",
    "\tplt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "\tplt.yticks(tick_marks, df_confusion.index)\n",
    "\t#plt.tight_layout()\n",
    "\tplt.ylabel(df_confusion.index.name)\n",
    "\tplt.xlabel(df_confusion.columns.name)\n",
    "\t\n",
    "\t\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "\t\"\"\"\n",
    "\tGiven X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "\t\n",
    "\tArguments:\n",
    "\tX -- input data containing sentences, numpy array of shape (m, None)\n",
    "\tY -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "\t\n",
    "\tReturns:\n",
    "\tpred -- numpy array of shape (m, 1) with your predictions\n",
    "\t\"\"\"\n",
    "\tm = X.shape[0]\n",
    "\tpred = np.zeros((m, 1))\n",
    "\t\n",
    "\tfor j in range(m):                       # Loop over training examples\n",
    "\t\t\n",
    "\t\t# Split jth test example (sentence) into list of lower case words\n",
    "\t\twords = X[j].lower().split()\n",
    "\t\t\n",
    "\t\t# Average words' vectors\n",
    "\t\tavg = np.zeros((50,))\n",
    "\t\tfor w in words:\n",
    "\t\t\tavg += word_to_vec_map[w]\n",
    "\t\tavg = avg/len(words)\n",
    "\n",
    "\t\t# Forward propagation\n",
    "\t\tZ = np.dot(W, avg) + b\n",
    "\t\tA = softmax(Z)\n",
    "\t\tpred[j] = np.argmax(A)\n",
    "\t\t\n",
    "\tprint(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "\t\n",
    "\treturn pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
